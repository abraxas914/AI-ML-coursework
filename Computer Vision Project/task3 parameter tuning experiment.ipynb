{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6e3086-cdfa-4776-82c4-9755cd737f95",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T12:15:33.434231Z",
     "iopub.status.busy": "2025-11-11T12:15:33.433869Z",
     "iopub.status.idle": "2025-11-11T12:32:32.836997Z",
     "shell.execute_reply": "2025-11-11T12:32:32.836365Z",
     "shell.execute_reply.started": "2025-11-11T12:15:33.434208Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "开始超参数调优实验...\n",
      "这将运行多组实验，可能需要较长时间，请耐心等待。\n",
      "\n",
      "\n",
      "\n",
      "######################################################################\n",
      "实验 1/7: 基准配置\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "开始训练 - 超参数配置:\n",
      "  学习率(lr): 0.001\n",
      "  训练轮数(num_epochs): 5\n",
      "  动量(momentum): 0.9\n",
      "  L2正则化(weight_decay): 0.01\n",
      "  Dropout率(dropout_rate): 0.5\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Batch  2000, Loss: 2.291\n",
      "Epoch 1, Batch  4000, Loss: 2.048\n",
      "Epoch 1, Batch  6000, Loss: 1.857\n",
      "Epoch 1, Batch  8000, Loss: 1.776\n",
      "Epoch 1, Batch 10000, Loss: 1.716\n",
      "Epoch 1, Batch 12000, Loss: 1.673\n",
      "测试集中的准确率为: 41 %\n",
      "Epoch 1 - 训练损失: 1.883, 测试准确率: 41.35%\n",
      "Epoch 2, Batch  2000, Loss: 1.606\n",
      "Epoch 2, Batch  4000, Loss: 1.596\n",
      "Epoch 2, Batch  6000, Loss: 1.585\n",
      "Epoch 2, Batch  8000, Loss: 1.558\n",
      "Epoch 2, Batch 10000, Loss: 1.570\n",
      "Epoch 2, Batch 12000, Loss: 1.569\n",
      "测试集中的准确率为: 47 %\n",
      "Epoch 2 - 训练损失: 1.579, 测试准确率: 47.09%\n",
      "Epoch 3, Batch  2000, Loss: 1.516\n",
      "Epoch 3, Batch  4000, Loss: 1.516\n",
      "Epoch 3, Batch  6000, Loss: 1.515\n",
      "Epoch 3, Batch  8000, Loss: 1.506\n",
      "Epoch 3, Batch 10000, Loss: 1.503\n",
      "Epoch 3, Batch 12000, Loss: 1.482\n",
      "测试集中的准确率为: 47 %\n",
      "Epoch 3 - 训练损失: 1.506, 测试准确率: 47.76%\n",
      "Epoch 4, Batch  2000, Loss: 1.468\n",
      "Epoch 4, Batch  4000, Loss: 1.458\n",
      "Epoch 4, Batch  6000, Loss: 1.472\n",
      "Epoch 4, Batch  8000, Loss: 1.468\n",
      "Epoch 4, Batch 10000, Loss: 1.450\n",
      "Epoch 4, Batch 12000, Loss: 1.451\n",
      "测试集中的准确率为: 49 %\n",
      "Epoch 4 - 训练损失: 1.460, 测试准确率: 49.10%\n",
      "Epoch 5, Batch  2000, Loss: 1.449\n",
      "Epoch 5, Batch  4000, Loss: 1.437\n",
      "Epoch 5, Batch  6000, Loss: 1.422\n",
      "Epoch 5, Batch  8000, Loss: 1.444\n",
      "Epoch 5, Batch 10000, Loss: 1.424\n",
      "Epoch 5, Batch 12000, Loss: 1.425\n",
      "测试集中的准确率为: 50 %\n",
      "Epoch 5 - 训练损失: 1.434, 测试准确率: 50.68%\n",
      "\n",
      "最终测试:\n",
      "测试集中的准确率为: 50 %\n",
      "\n",
      "\n",
      "######################################################################\n",
      "实验 2/7: 高学习率\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "开始训练 - 超参数配置:\n",
      "  学习率(lr): 0.01\n",
      "  训练轮数(num_epochs): 5\n",
      "  动量(momentum): 0.9\n",
      "  L2正则化(weight_decay): 0.01\n",
      "  Dropout率(dropout_rate): 0.5\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Batch  2000, Loss: 2.149\n",
      "Epoch 1, Batch  4000, Loss: 2.062\n",
      "Epoch 1, Batch  6000, Loss: 2.043\n",
      "Epoch 1, Batch  8000, Loss: 2.050\n",
      "Epoch 1, Batch 10000, Loss: 2.037\n",
      "Epoch 1, Batch 12000, Loss: 2.054\n",
      "测试集中的准确率为: 19 %\n",
      "Epoch 1 - 训练损失: 2.065, 测试准确率: 19.47%\n",
      "Epoch 2, Batch  2000, Loss: 2.054\n",
      "Epoch 2, Batch  4000, Loss: 2.049\n",
      "Epoch 2, Batch  6000, Loss: 2.044\n",
      "Epoch 2, Batch  8000, Loss: 2.058\n",
      "Epoch 2, Batch 10000, Loss: 2.076\n",
      "Epoch 2, Batch 12000, Loss: 2.054\n",
      "测试集中的准确率为: 22 %\n",
      "Epoch 2 - 训练损失: 2.057, 测试准确率: 22.37%\n",
      "Epoch 3, Batch  2000, Loss: 2.052\n",
      "Epoch 3, Batch  4000, Loss: 2.050\n",
      "Epoch 3, Batch  6000, Loss: 2.074\n",
      "Epoch 3, Batch  8000, Loss: 2.061\n",
      "Epoch 3, Batch 10000, Loss: 2.051\n",
      "Epoch 3, Batch 12000, Loss: 2.060\n",
      "测试集中的准确率为: 21 %\n",
      "Epoch 3 - 训练损失: 2.057, 测试准确率: 21.71%\n",
      "Epoch 4, Batch  2000, Loss: 2.055\n",
      "Epoch 4, Batch  4000, Loss: 2.054\n",
      "Epoch 4, Batch  6000, Loss: 2.049\n",
      "Epoch 4, Batch  8000, Loss: 2.060\n",
      "Epoch 4, Batch 10000, Loss: 2.051\n",
      "Epoch 4, Batch 12000, Loss: 2.064\n",
      "测试集中的准确率为: 21 %\n",
      "Epoch 4 - 训练损失: 2.056, 测试准确率: 21.02%\n",
      "Epoch 5, Batch  2000, Loss: 2.063\n",
      "Epoch 5, Batch  4000, Loss: 2.090\n",
      "Epoch 5, Batch  6000, Loss: 2.057\n",
      "Epoch 5, Batch  8000, Loss: 2.058\n",
      "Epoch 5, Batch 10000, Loss: 2.061\n",
      "Epoch 5, Batch 12000, Loss: 2.054\n",
      "测试集中的准确率为: 22 %\n",
      "Epoch 5 - 训练损失: 2.063, 测试准确率: 22.29%\n",
      "\n",
      "最终测试:\n",
      "测试集中的准确率为: 22 %\n",
      "\n",
      "\n",
      "######################################################################\n",
      "实验 3/7: 低学习率\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "开始训练 - 超参数配置:\n",
      "  学习率(lr): 0.0001\n",
      "  训练轮数(num_epochs): 5\n",
      "  动量(momentum): 0.9\n",
      "  L2正则化(weight_decay): 0.01\n",
      "  Dropout率(dropout_rate): 0.5\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Batch  2000, Loss: 2.304\n",
      "Epoch 1, Batch  4000, Loss: 2.301\n",
      "Epoch 1, Batch  6000, Loss: 2.299\n",
      "Epoch 1, Batch  8000, Loss: 2.294\n",
      "Epoch 1, Batch 10000, Loss: 2.287\n",
      "Epoch 1, Batch 12000, Loss: 2.262\n",
      "测试集中的准确率为: 20 %\n",
      "Epoch 1 - 训练损失: 2.289, 测试准确率: 20.21%\n",
      "Epoch 2, Batch  2000, Loss: 2.180\n",
      "Epoch 2, Batch  4000, Loss: 2.120\n",
      "Epoch 2, Batch  6000, Loss: 2.079\n",
      "Epoch 2, Batch  8000, Loss: 2.037\n",
      "Epoch 2, Batch 10000, Loss: 2.003\n",
      "Epoch 2, Batch 12000, Loss: 1.962\n",
      "测试集中的准确率为: 31 %\n",
      "Epoch 2 - 训练损失: 2.059, 测试准确率: 31.11%\n",
      "Epoch 3, Batch  2000, Loss: 1.926\n",
      "Epoch 3, Batch  4000, Loss: 1.889\n",
      "Epoch 3, Batch  6000, Loss: 1.870\n",
      "Epoch 3, Batch  8000, Loss: 1.862\n",
      "Epoch 3, Batch 10000, Loss: 1.825\n",
      "Epoch 3, Batch 12000, Loss: 1.812\n",
      "测试集中的准确率为: 34 %\n",
      "Epoch 3 - 训练损失: 1.862, 测试准确率: 34.43%\n",
      "Epoch 4, Batch  2000, Loss: 1.798\n",
      "Epoch 4, Batch  4000, Loss: 1.779\n",
      "Epoch 4, Batch  6000, Loss: 1.753\n",
      "Epoch 4, Batch  8000, Loss: 1.758\n",
      "Epoch 4, Batch 10000, Loss: 1.753\n",
      "Epoch 4, Batch 12000, Loss: 1.728\n",
      "测试集中的准确率为: 37 %\n",
      "Epoch 4 - 训练损失: 1.760, 测试准确率: 37.81%\n",
      "Epoch 5, Batch  2000, Loss: 1.724\n",
      "Epoch 5, Batch  4000, Loss: 1.698\n",
      "Epoch 5, Batch  6000, Loss: 1.705\n",
      "Epoch 5, Batch  8000, Loss: 1.700\n",
      "Epoch 5, Batch 10000, Loss: 1.671\n",
      "Epoch 5, Batch 12000, Loss: 1.670\n",
      "测试集中的准确率为: 40 %\n",
      "Epoch 5 - 训练损失: 1.692, 测试准确率: 40.21%\n",
      "\n",
      "最终测试:\n",
      "测试集中的准确率为: 40 %\n",
      "\n",
      "\n",
      "######################################################################\n",
      "实验 4/7: 更多epoch\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "开始训练 - 超参数配置:\n",
      "  学习率(lr): 0.001\n",
      "  训练轮数(num_epochs): 10\n",
      "  动量(momentum): 0.9\n",
      "  L2正则化(weight_decay): 0.01\n",
      "  Dropout率(dropout_rate): 0.5\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Batch  2000, Loss: 2.260\n",
      "Epoch 1, Batch  4000, Loss: 2.025\n",
      "Epoch 1, Batch  6000, Loss: 1.911\n",
      "Epoch 1, Batch  8000, Loss: 1.807\n",
      "Epoch 1, Batch 10000, Loss: 1.720\n",
      "Epoch 1, Batch 12000, Loss: 1.684\n",
      "测试集中的准确率为: 40 %\n",
      "Epoch 1 - 训练损失: 1.892, 测试准确率: 40.81%\n",
      "Epoch 2, Batch  2000, Loss: 1.622\n",
      "Epoch 2, Batch  4000, Loss: 1.616\n",
      "Epoch 2, Batch  6000, Loss: 1.604\n",
      "Epoch 2, Batch  8000, Loss: 1.568\n",
      "Epoch 2, Batch 10000, Loss: 1.559\n",
      "Epoch 2, Batch 12000, Loss: 1.523\n",
      "测试集中的准确率为: 45 %\n",
      "Epoch 2 - 训练损失: 1.580, 测试准确率: 45.56%\n",
      "Epoch 3, Batch  2000, Loss: 1.506\n",
      "Epoch 3, Batch  4000, Loss: 1.476\n",
      "Epoch 3, Batch  6000, Loss: 1.515\n",
      "Epoch 3, Batch  8000, Loss: 1.498\n",
      "Epoch 3, Batch 10000, Loss: 1.469\n",
      "Epoch 3, Batch 12000, Loss: 1.463\n",
      "测试集中的准确率为: 49 %\n",
      "Epoch 3 - 训练损失: 1.487, 测试准确率: 49.10%\n",
      "Epoch 4, Batch  2000, Loss: 1.449\n",
      "Epoch 4, Batch  4000, Loss: 1.453\n",
      "Epoch 4, Batch  6000, Loss: 1.430\n",
      "Epoch 4, Batch  8000, Loss: 1.457\n",
      "Epoch 4, Batch 10000, Loss: 1.436\n",
      "Epoch 4, Batch 12000, Loss: 1.447\n",
      "测试集中的准确率为: 49 %\n",
      "Epoch 4 - 训练损失: 1.445, 测试准确率: 49.48%\n",
      "Epoch 5, Batch  2000, Loss: 1.417\n",
      "Epoch 5, Batch  4000, Loss: 1.434\n",
      "Epoch 5, Batch  6000, Loss: 1.421\n",
      "Epoch 5, Batch  8000, Loss: 1.433\n",
      "Epoch 5, Batch 10000, Loss: 1.416\n",
      "Epoch 5, Batch 12000, Loss: 1.420\n",
      "测试集中的准确率为: 50 %\n",
      "Epoch 5 - 训练损失: 1.424, 测试准确率: 50.88%\n",
      "Epoch 6, Batch  2000, Loss: 1.407\n",
      "Epoch 6, Batch  4000, Loss: 1.410\n",
      "Epoch 6, Batch  6000, Loss: 1.391\n",
      "Epoch 6, Batch  8000, Loss: 1.430\n",
      "Epoch 6, Batch 10000, Loss: 1.404\n",
      "Epoch 6, Batch 12000, Loss: 1.412\n",
      "测试集中的准确率为: 52 %\n",
      "Epoch 6 - 训练损失: 1.408, 测试准确率: 52.19%\n",
      "Epoch 7, Batch  2000, Loss: 1.407\n",
      "Epoch 7, Batch  4000, Loss: 1.382\n",
      "Epoch 7, Batch  6000, Loss: 1.413\n",
      "Epoch 7, Batch  8000, Loss: 1.391\n",
      "Epoch 7, Batch 10000, Loss: 1.408\n",
      "Epoch 7, Batch 12000, Loss: 1.398\n",
      "测试集中的准确率为: 52 %\n",
      "Epoch 7 - 训练损失: 1.401, 测试准确率: 52.75%\n",
      "Epoch 8, Batch  2000, Loss: 1.387\n",
      "Epoch 8, Batch  4000, Loss: 1.393\n",
      "Epoch 8, Batch  6000, Loss: 1.391\n",
      "Epoch 8, Batch  8000, Loss: 1.378\n",
      "Epoch 8, Batch 10000, Loss: 1.410\n",
      "Epoch 8, Batch 12000, Loss: 1.379\n",
      "测试集中的准确率为: 52 %\n",
      "Epoch 8 - 训练损失: 1.389, 测试准确率: 52.50%\n",
      "Epoch 9, Batch  2000, Loss: 1.383\n",
      "Epoch 9, Batch  4000, Loss: 1.373\n",
      "Epoch 9, Batch  6000, Loss: 1.376\n",
      "Epoch 9, Batch  8000, Loss: 1.386\n",
      "Epoch 9, Batch 10000, Loss: 1.384\n",
      "Epoch 9, Batch 12000, Loss: 1.385\n",
      "测试集中的准确率为: 50 %\n",
      "Epoch 9 - 训练损失: 1.380, 测试准确率: 50.47%\n",
      "Epoch 10, Batch  2000, Loss: 1.381\n",
      "Epoch 10, Batch  4000, Loss: 1.368\n",
      "Epoch 10, Batch  6000, Loss: 1.377\n",
      "Epoch 10, Batch  8000, Loss: 1.369\n",
      "Epoch 10, Batch 10000, Loss: 1.371\n",
      "Epoch 10, Batch 12000, Loss: 1.396\n",
      "测试集中的准确率为: 54 %\n",
      "Epoch 10 - 训练损失: 1.379, 测试准确率: 54.93%\n",
      "\n",
      "最终测试:\n",
      "测试集中的准确率为: 54 %\n",
      "\n",
      "\n",
      "######################################################################\n",
      "实验 5/7: 无正则化\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "开始训练 - 超参数配置:\n",
      "  学习率(lr): 0.001\n",
      "  训练轮数(num_epochs): 5\n",
      "  动量(momentum): 0.9\n",
      "  L2正则化(weight_decay): 0\n",
      "  Dropout率(dropout_rate): 0\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Batch  2000, Loss: 2.178\n",
      "Epoch 1, Batch  4000, Loss: 1.851\n",
      "Epoch 1, Batch  6000, Loss: 1.635\n",
      "Epoch 1, Batch  8000, Loss: 1.567\n",
      "Epoch 1, Batch 10000, Loss: 1.487\n",
      "Epoch 1, Batch 12000, Loss: 1.453\n",
      "测试集中的准确率为: 50 %\n",
      "Epoch 1 - 训练损失: 1.683, 测试准确率: 50.60%\n",
      "Epoch 2, Batch  2000, Loss: 1.392\n",
      "Epoch 2, Batch  4000, Loss: 1.346\n",
      "Epoch 2, Batch  6000, Loss: 1.342\n",
      "Epoch 2, Batch  8000, Loss: 1.322\n",
      "Epoch 2, Batch 10000, Loss: 1.289\n",
      "Epoch 2, Batch 12000, Loss: 1.282\n",
      "测试集中的准确率为: 56 %\n",
      "Epoch 2 - 训练损失: 1.326, 测试准确率: 56.50%\n",
      "Epoch 3, Batch  2000, Loss: 1.206\n",
      "Epoch 3, Batch  4000, Loss: 1.197\n",
      "Epoch 3, Batch  6000, Loss: 1.198\n",
      "Epoch 3, Batch  8000, Loss: 1.162\n",
      "Epoch 3, Batch 10000, Loss: 1.171\n",
      "Epoch 3, Batch 12000, Loss: 1.165\n",
      "测试集中的准确率为: 55 %\n",
      "Epoch 3 - 训练损失: 1.183, 测试准确率: 55.92%\n",
      "Epoch 4, Batch  2000, Loss: 1.092\n",
      "Epoch 4, Batch  4000, Loss: 1.094\n",
      "Epoch 4, Batch  6000, Loss: 1.106\n",
      "Epoch 4, Batch  8000, Loss: 1.083\n",
      "Epoch 4, Batch 10000, Loss: 1.106\n",
      "Epoch 4, Batch 12000, Loss: 1.079\n",
      "测试集中的准确率为: 60 %\n",
      "Epoch 4 - 训练损失: 1.092, 测试准确率: 60.53%\n",
      "Epoch 5, Batch  2000, Loss: 1.024\n",
      "Epoch 5, Batch  4000, Loss: 1.032\n",
      "Epoch 5, Batch  6000, Loss: 1.016\n",
      "Epoch 5, Batch  8000, Loss: 1.030\n",
      "Epoch 5, Batch 10000, Loss: 1.007\n",
      "Epoch 5, Batch 12000, Loss: 1.033\n",
      "测试集中的准确率为: 61 %\n",
      "Epoch 5 - 训练损失: 1.024, 测试准确率: 61.35%\n",
      "\n",
      "最终测试:\n",
      "测试集中的准确率为: 61 %\n",
      "\n",
      "\n",
      "######################################################################\n",
      "实验 6/7: 强正则化\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "开始训练 - 超参数配置:\n",
      "  学习率(lr): 0.001\n",
      "  训练轮数(num_epochs): 5\n",
      "  动量(momentum): 0.9\n",
      "  L2正则化(weight_decay): 0.1\n",
      "  Dropout率(dropout_rate): 0.7\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Batch  2000, Loss: 2.303\n",
      "Epoch 1, Batch  4000, Loss: 2.303\n",
      "Epoch 1, Batch  6000, Loss: 2.303\n",
      "Epoch 1, Batch  8000, Loss: 2.303\n",
      "Epoch 1, Batch 10000, Loss: 2.303\n",
      "Epoch 1, Batch 12000, Loss: 2.303\n",
      "测试集中的准确率为: 10 %\n",
      "Epoch 1 - 训练损失: 2.303, 测试准确率: 10.00%\n",
      "Epoch 2, Batch  2000, Loss: 2.302\n",
      "Epoch 2, Batch  4000, Loss: 2.303\n",
      "Epoch 2, Batch  6000, Loss: 2.303\n",
      "Epoch 2, Batch  8000, Loss: 2.303\n",
      "Epoch 2, Batch 10000, Loss: 2.303\n",
      "Epoch 2, Batch 12000, Loss: 2.303\n",
      "测试集中的准确率为: 10 %\n",
      "Epoch 2 - 训练损失: 2.303, 测试准确率: 10.00%\n",
      "Epoch 3, Batch  2000, Loss: 2.303\n",
      "Epoch 3, Batch  4000, Loss: 2.303\n",
      "Epoch 3, Batch  6000, Loss: 2.303\n",
      "Epoch 3, Batch  8000, Loss: 2.303\n",
      "Epoch 3, Batch 10000, Loss: 2.303\n",
      "Epoch 3, Batch 12000, Loss: 2.303\n",
      "测试集中的准确率为: 10 %\n",
      "Epoch 3 - 训练损失: 2.303, 测试准确率: 10.00%\n",
      "Epoch 4, Batch  2000, Loss: 2.303\n",
      "Epoch 4, Batch  4000, Loss: 2.303\n",
      "Epoch 4, Batch  6000, Loss: 2.303\n",
      "Epoch 4, Batch  8000, Loss: 2.303\n",
      "Epoch 4, Batch 10000, Loss: 2.303\n",
      "Epoch 4, Batch 12000, Loss: 2.303\n",
      "测试集中的准确率为: 10 %\n",
      "Epoch 4 - 训练损失: 2.303, 测试准确率: 10.00%\n",
      "Epoch 5, Batch  2000, Loss: 2.303\n",
      "Epoch 5, Batch  4000, Loss: 2.303\n",
      "Epoch 5, Batch  6000, Loss: 2.303\n",
      "Epoch 5, Batch  8000, Loss: 2.303\n",
      "Epoch 5, Batch 10000, Loss: 2.303\n",
      "Epoch 5, Batch 12000, Loss: 2.303\n",
      "测试集中的准确率为: 10 %\n",
      "Epoch 5 - 训练损失: 2.303, 测试准确率: 10.00%\n",
      "\n",
      "最终测试:\n",
      "测试集中的准确率为: 10 %\n",
      "\n",
      "\n",
      "######################################################################\n",
      "实验 7/7: 无动量\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "开始训练 - 超参数配置:\n",
      "  学习率(lr): 0.001\n",
      "  训练轮数(num_epochs): 5\n",
      "  动量(momentum): 0\n",
      "  L2正则化(weight_decay): 0.01\n",
      "  Dropout率(dropout_rate): 0.5\n",
      "======================================================================\n",
      "\n",
      "Epoch 1, Batch  2000, Loss: 2.303\n",
      "Epoch 1, Batch  4000, Loss: 2.300\n",
      "Epoch 1, Batch  6000, Loss: 2.295\n",
      "Epoch 1, Batch  8000, Loss: 2.283\n",
      "Epoch 1, Batch 10000, Loss: 2.263\n",
      "Epoch 1, Batch 12000, Loss: 2.230\n",
      "测试集中的准确率为: 18 %\n",
      "Epoch 1 - 训练损失: 2.276, 测试准确率: 18.43%\n",
      "Epoch 2, Batch  2000, Loss: 2.195\n",
      "Epoch 2, Batch  4000, Loss: 2.175\n",
      "Epoch 2, Batch  6000, Loss: 2.136\n",
      "Epoch 2, Batch  8000, Loss: 2.092\n",
      "Epoch 2, Batch 10000, Loss: 2.075\n",
      "Epoch 2, Batch 12000, Loss: 2.031\n",
      "测试集中的准确率为: 25 %\n",
      "Epoch 2 - 训练损失: 2.114, 测试准确率: 25.64%\n",
      "Epoch 3, Batch  2000, Loss: 1.988\n",
      "Epoch 3, Batch  4000, Loss: 1.994\n",
      "Epoch 3, Batch  6000, Loss: 1.953\n",
      "Epoch 3, Batch  8000, Loss: 1.939\n",
      "Epoch 3, Batch 10000, Loss: 1.902\n",
      "Epoch 3, Batch 12000, Loss: 1.882\n",
      "测试集中的准确率为: 31 %\n",
      "Epoch 3 - 训练损失: 1.940, 测试准确率: 31.49%\n",
      "Epoch 4, Batch  2000, Loss: 1.866\n",
      "Epoch 4, Batch  4000, Loss: 1.844\n",
      "Epoch 4, Batch  6000, Loss: 1.824\n",
      "Epoch 4, Batch  8000, Loss: 1.810\n",
      "Epoch 4, Batch 10000, Loss: 1.809\n",
      "Epoch 4, Batch 12000, Loss: 1.775\n",
      "测试集中的准确率为: 35 %\n",
      "Epoch 4 - 训练损失: 1.820, 测试准确率: 35.20%\n",
      "Epoch 5, Batch  2000, Loss: 1.767\n",
      "Epoch 5, Batch  4000, Loss: 1.763\n",
      "Epoch 5, Batch  6000, Loss: 1.750\n",
      "Epoch 5, Batch  8000, Loss: 1.739\n",
      "Epoch 5, Batch 10000, Loss: 1.734\n",
      "Epoch 5, Batch 12000, Loss: 1.723\n",
      "测试集中的准确率为: 36 %\n",
      "Epoch 5 - 训练损失: 1.745, 测试准确率: 36.51%\n",
      "\n",
      "最终测试:\n",
      "测试集中的准确率为: 36 %\n",
      "\n",
      "所有实验结果已保存至: ./hyperparameter_experiments/all_results.json\n",
      "\n",
      "\n",
      "生成可视化结果...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 20934 (\\N{CJK UNIFIED IDEOGRAPH-51C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 37197 (\\N{CJK UNIFIED IDEOGRAPH-914D}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 23398 (\\N{CJK UNIFIED IDEOGRAPH-5B66}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 20064 (\\N{CJK UNIFIED IDEOGRAPH-4E60}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 20302 (\\N{CJK UNIFIED IDEOGRAPH-4F4E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 26356 (\\N{CJK UNIFIED IDEOGRAPH-66F4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 22810 (\\N{CJK UNIFIED IDEOGRAPH-591A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 26080 (\\N{CJK UNIFIED IDEOGRAPH-65E0}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 27491 (\\N{CJK UNIFIED IDEOGRAPH-6B63}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 21017 (\\N{CJK UNIFIED IDEOGRAPH-5219}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 24378 (\\N{CJK UNIFIED IDEOGRAPH-5F3A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 21160 (\\N{CJK UNIFIED IDEOGRAPH-52A8}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:348: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 20934 (\\N{CJK UNIFIED IDEOGRAPH-51C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 37197 (\\N{CJK UNIFIED IDEOGRAPH-914D}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 23398 (\\N{CJK UNIFIED IDEOGRAPH-5B66}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 20064 (\\N{CJK UNIFIED IDEOGRAPH-4E60}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 20302 (\\N{CJK UNIFIED IDEOGRAPH-4F4E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 26356 (\\N{CJK UNIFIED IDEOGRAPH-66F4}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 22810 (\\N{CJK UNIFIED IDEOGRAPH-591A}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 26080 (\\N{CJK UNIFIED IDEOGRAPH-65E0}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 27491 (\\N{CJK UNIFIED IDEOGRAPH-6B63}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 21017 (\\N{CJK UNIFIED IDEOGRAPH-5219}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 24378 (\\N{CJK UNIFIED IDEOGRAPH-5F3A}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 21160 (\\N{CJK UNIFIED IDEOGRAPH-52A8}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:350: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "对比图已保存至: ./hyperparameter_experiments/hyperparameter_comparison.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 20934 (\\N{CJK UNIFIED IDEOGRAPH-51C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 37197 (\\N{CJK UNIFIED IDEOGRAPH-914D}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 26368 (\\N{CJK UNIFIED IDEOGRAPH-6700}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 32456 (\\N{CJK UNIFIED IDEOGRAPH-7EC8}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 30830 (\\N{CJK UNIFIED IDEOGRAPH-786E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 23398 (\\N{CJK UNIFIED IDEOGRAPH-5B66}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 20064 (\\N{CJK UNIFIED IDEOGRAPH-4E60}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 20302 (\\N{CJK UNIFIED IDEOGRAPH-4F4E}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 26356 (\\N{CJK UNIFIED IDEOGRAPH-66F4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 22810 (\\N{CJK UNIFIED IDEOGRAPH-591A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 26080 (\\N{CJK UNIFIED IDEOGRAPH-65E0}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 27491 (\\N{CJK UNIFIED IDEOGRAPH-6B63}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 21017 (\\N{CJK UNIFIED IDEOGRAPH-5219}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:376: UserWarning: Glyph 24378 (\\N{CJK UNIFIED IDEOGRAPH-5F3A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 22522 (\\N{CJK UNIFIED IDEOGRAPH-57FA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 20934 (\\N{CJK UNIFIED IDEOGRAPH-51C6}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 37197 (\\N{CJK UNIFIED IDEOGRAPH-914D}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 26368 (\\N{CJK UNIFIED IDEOGRAPH-6700}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 32456 (\\N{CJK UNIFIED IDEOGRAPH-7EC8}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 30830 (\\N{CJK UNIFIED IDEOGRAPH-786E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 23398 (\\N{CJK UNIFIED IDEOGRAPH-5B66}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 20064 (\\N{CJK UNIFIED IDEOGRAPH-4E60}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 20302 (\\N{CJK UNIFIED IDEOGRAPH-4F4E}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 26356 (\\N{CJK UNIFIED IDEOGRAPH-66F4}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 22810 (\\N{CJK UNIFIED IDEOGRAPH-591A}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 26080 (\\N{CJK UNIFIED IDEOGRAPH-65E0}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 27491 (\\N{CJK UNIFIED IDEOGRAPH-6B63}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 21017 (\\N{CJK UNIFIED IDEOGRAPH-5219}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
      "/tmp/ipykernel_650/2223991275.py:378: UserWarning: Glyph 24378 (\\N{CJK UNIFIED IDEOGRAPH-5F3A}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "详细训练曲线已保存至: ./hyperparameter_experiments/detailed_training_curves.png\n",
      "\n",
      "================================================================================\n",
      "实验结果汇总表\n",
      "================================================================================\n",
      "实验名称            学习率        Epoch    L2正则       Dropout    最终损失         最终准确率       \n",
      "--------------------------------------------------------------------------------\n",
      "基准配置            0.0010     5        0.0100     0.50       1.4336       50.68       %\n",
      "高学习率            0.0100     5        0.0100     0.50       2.0633       22.29       %\n",
      "低学习率            0.0001     5        0.0100     0.50       1.6921       40.21       %\n",
      "更多epoch         0.0010     10       0.0100     0.50       1.3792       54.93       %\n",
      "无正则化            0.0010     5        0.0000     0.00       1.0241       61.35       %\n",
      "强正则化            0.0010     5        0.1000     0.70       2.3029       10.00       %\n",
      "无动量             0.0010     5        0.0100     0.50       1.7448       36.51       %\n",
      "================================================================================\n",
      "\n",
      "实验完成！\n",
      "所有结果已保存在 './hyperparameter_experiments' 目录下\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set non-interactive backend for matplotlib (suitable for headless servers like Alibaba Cloud)\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "# Device configuration: Use GPU if available, else CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 设定对图片的归一化处理方式\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 4  # Consider increasing to 32-128 for better GPU utilization\n",
    "\n",
    "# 下载数据集\n",
    "trainset = torchvision.datasets.CIFAR10(root='./dataset', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./dataset', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "# 网络结构定义\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def predict(testloader, net, device):\n",
    "    \"\"\"\n",
    "    测试函数，评估模型在测试集上的准确率\n",
    "    \n",
    "    参数:\n",
    "        testloader: 测试数据加载器\n",
    "        net: 神经网络模型\n",
    "        device: 计算设备 (GPU or CPU)\n",
    "    \n",
    "    返回:\n",
    "        accuracy: 测试集准确率\n",
    "    \"\"\"\n",
    "    correct = 0  # 预测正确的图片数\n",
    "    total = 0    # 总共的图片数\n",
    "    \n",
    "    # 设置为评估模式（Dropout会失效）\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():  # 正向传播时不计算梯度\n",
    "        for data in testloader:\n",
    "            # 1. 取出数据并移动到设备\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # 2. 正向传播，得到输出结果\n",
    "            outputs = net(images)\n",
    "            # 3. 从输出中得到模型预测\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # 4. 计算性能指标\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy = 100 * correct.item() / total  # 使用 .item() 以避免 Tensor 类型\n",
    "    print('测试集中的准确率为: %d %%' % accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_and_evaluate(trainloader, testloader, hyperparams, save_path, device):\n",
    "    \"\"\"\n",
    "    使用指定的超参数训练模型并评估\n",
    "    \n",
    "    参数:\n",
    "        trainloader: 训练数据加载器\n",
    "        testloader: 测试数据加载器\n",
    "        hyperparams: 超参数字典\n",
    "        save_path: 结果保存路径\n",
    "        device: 计算设备 (GPU or CPU)\n",
    "    \n",
    "    返回:\n",
    "        results: 包含训练和测试结果的字典\n",
    "    \"\"\"\n",
    "    # 解析超参数\n",
    "    num_epochs = hyperparams['num_epochs']\n",
    "    lr = hyperparams['learning_rate']\n",
    "    momentum = hyperparams['momentum']\n",
    "    weight_decay = hyperparams['weight_decay']\n",
    "    dropout_rate = hyperparams['dropout_rate']\n",
    "    \n",
    "    # 创建模型并移动到设备\n",
    "    net = Net(dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, \n",
    "                         weight_decay=weight_decay)\n",
    "    \n",
    "    # 记录训练过程\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    epoch_train_losses = []\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"开始训练 - 超参数配置:\")\n",
    "    print(f\"  学习率(lr): {lr}\")\n",
    "    print(f\"  训练轮数(num_epochs): {num_epochs}\")\n",
    "    print(f\"  动量(momentum): {momentum}\")\n",
    "    print(f\"  L2正则化(weight_decay): {weight_decay}\")\n",
    "    print(f\"  Dropout率(dropout_rate): {dropout_rate}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # 数据移动到设备\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            current_loss = loss.item()\n",
    "            running_loss += current_loss\n",
    "            epoch_loss += current_loss\n",
    "            num_batches += 1\n",
    "            train_losses.append(current_loss)\n",
    "            \n",
    "            if i % 2000 == 1999:\n",
    "                avg_loss = running_loss / 2000\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1:5d}, Loss: {avg_loss:.3f}')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # 记录epoch平均损失\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        epoch_train_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        # 测试阶段 - 使用predict函数\n",
    "        accuracy = predict(testloader, net, device)\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1} - 训练损失: {avg_epoch_loss:.3f}, '\n",
    "              f'测试准确率: {accuracy:.2f}%')\n",
    "    \n",
    "    # 最终测试 - 使用predict函数\n",
    "    print('\\n最终测试:')\n",
    "    final_accuracy = predict(testloader, net, device)\n",
    "    \n",
    "    # 保存结果\n",
    "    results = {\n",
    "        'hyperparams': hyperparams,\n",
    "        'train_losses': train_losses,\n",
    "        'epoch_train_losses': epoch_train_losses,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'final_accuracy': final_accuracy\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_hyperparameter_experiments(device):\n",
    "    \"\"\"\n",
    "    运行多组超参数实验\n",
    "    \"\"\"\n",
    "    # 定义要测试的超参数组合\n",
    "    experiments = [\n",
    "        # 基准实验\n",
    "        {\n",
    "            'name': 'benchmark_configuration',\n",
    "            'num_epochs': 5,\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 0.01,\n",
    "            'dropout_rate': 0.5\n",
    "        },\n",
    "        # 实验1: 增加学习率\n",
    "        {\n",
    "            'name': 'high_learning_rate',\n",
    "            'num_epochs': 5,\n",
    "            'learning_rate': 0.01,  # 10倍学习率\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 0.01,\n",
    "            'dropout_rate': 0.5\n",
    "        },\n",
    "        # 实验2: 降低学习率\n",
    "        {\n",
    "            'name': 'low_learning_rate',\n",
    "            'num_epochs': 5,\n",
    "            'learning_rate': 0.0001,  # 0.1倍学习率\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 0.01,\n",
    "            'dropout_rate': 0.5\n",
    "        },\n",
    "        # 实验3: 更多训练轮数\n",
    "        {\n",
    "            'name': 'more epoch',\n",
    "            'num_epochs': 10,  # 2倍epoch\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 0.01,\n",
    "            'dropout_rate': 0.5\n",
    "        },\n",
    "        # 实验4: 无正则化\n",
    "        {\n",
    "            'name': 'without regularization',\n",
    "            'num_epochs': 5,\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 0,  # 无L2正则化\n",
    "            'dropout_rate': 0    # 无Dropout\n",
    "        },\n",
    "        # 实验5: 强正则化\n",
    "        {\n",
    "            'name': 'strengthend regularization',\n",
    "            'num_epochs': 5,\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 0.1,  # 10倍L2正则化\n",
    "            'dropout_rate': 0.7   # 更高的Dropout率\n",
    "        },\n",
    "        # 实验6: 无动量\n",
    "        {\n",
    "            'name': 'without momentum',\n",
    "            'num_epochs': 5,\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0,  # 无动量\n",
    "            'weight_decay': 0.01,\n",
    "            'dropout_rate': 0.5\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 运行所有实验\n",
    "    all_results = []\n",
    "    save_path = './hyperparameter_experiments'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for i, exp in enumerate(experiments):\n",
    "        print(f\"\\n\\n{'#'*70}\")\n",
    "        print(f\"实验 {i+1}/{len(experiments)}: {exp['name']}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        results = train_and_evaluate(trainloader, testloader, exp, save_path, device)\n",
    "        results['experiment_name'] = exp['name']\n",
    "        all_results.append(results)\n",
    "    \n",
    "    # 保存所有实验结果\n",
    "    results_file = os.path.join(save_path, 'all_results.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"\\n所有实验结果已保存至: {results_file}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def visualize_results(all_results, save_path='./hyperparameter_experiments'):\n",
    "    \"\"\"\n",
    "    可视化所有实验结果\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 图1: 对比不同配置的训练损失\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 子图1: epoch级别的训练损失对比\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for result in all_results:\n",
    "        epochs = range(1, len(result['epoch_train_losses']) + 1)\n",
    "        plt.plot(epochs, result['epoch_train_losses'], \n",
    "                marker='o', label=result['experiment_name'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Training Loss')\n",
    "    plt.title('train loss comparison (each Epoch)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 子图2: 测试准确率对比\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for result in all_results:\n",
    "        epochs = range(1, len(result['test_accuracies']) + 1)\n",
    "        plt.plot(epochs, result['test_accuracies'], \n",
    "                marker='s', label=result['experiment_name'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test Accuracy (%)')\n",
    "    plt.title('test accuracy comparison')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 子图3: 最终测试准确率柱状图\n",
    "    plt.subplot(2, 2, 3)\n",
    "    names = [r['experiment_name'] for r in all_results]\n",
    "    final_accs = [r['final_accuracy'] for r in all_results]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n",
    "    bars = plt.bar(range(len(names)), final_accs, color=colors)\n",
    "    plt.xlabel('experiment configurations')\n",
    "    plt.ylabel('Final Test Accuracy (%)')\n",
    "    plt.title('Final Test Accuracy Comparison')\n",
    "    plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 在柱状图上标注数值\n",
    "    for i, (bar, acc) in enumerate(zip(bars, final_accs)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{acc:.2f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 子图4: 最终损失 vs 最终准确率散点图\n",
    "    plt.subplot(2, 2, 4)\n",
    "    final_losses = [r['epoch_train_losses'][-1] for r in all_results]\n",
    "    plt.scatter(final_losses, final_accs, s=200, c=colors, alpha=0.6)\n",
    "    for i, name in enumerate(names):\n",
    "        plt.annotate(name, (final_losses[i], final_accs[i]), \n",
    "                    fontsize=8, ha='right')\n",
    "    plt.xlabel('final training loss')\n",
    "    plt.ylabel('final test accuracy (%)')\n",
    "    plt.title('Final Loss vs Final Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_path, 'hyperparameter_comparison.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f'\\n对比图已保存至: {plot_path}')\n",
    "    plt.close()  # Close figure to free memory on server\n",
    "    \n",
    "    # 图2: 详细的训练曲线（仅显示前3个实验，避免过于拥挤）\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, result in enumerate(all_results[:6]):  # 最多显示6个\n",
    "        # 训练损失（使用移动平均）\n",
    "        train_losses = result['train_losses']\n",
    "        if len(train_losses) > 100:\n",
    "            window_size = 100\n",
    "            moving_avg = np.convolve(train_losses, \n",
    "                                    np.ones(window_size)/window_size, \n",
    "                                    mode='valid')\n",
    "            axes[i].plot(moving_avg, color='blue', alpha=0.8, linewidth=1.5)\n",
    "        else:\n",
    "            axes[i].plot(train_losses, color='blue', alpha=0.8, linewidth=1.5)\n",
    "        \n",
    "        axes[i].set_title(f\"{result['experiment_name']}\\n\"\n",
    "                         f\"最终准确率: {result['final_accuracy']:.2f}%\")\n",
    "        axes[i].set_xlabel('Batch')\n",
    "        axes[i].set_ylabel('Loss')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    detail_plot_path = os.path.join(save_path, 'detailed_training_curves.png')\n",
    "    plt.savefig(detail_plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f'详细训练曲线已保存至: {detail_plot_path}')\n",
    "    plt.close()  # Close figure\n",
    "    \n",
    "    # 生成实验结果汇总表\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"实验结果汇总表\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'实验名称':<15} {'学习率':<10} {'Epoch':<8} {'L2正则':<10} \"\n",
    "          f\"{'Dropout':<10} {'最终损失':<12} {'最终准确率':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for result in all_results:\n",
    "        hp = result['hyperparams']\n",
    "        print(f\"{result['experiment_name']:<15} \"\n",
    "              f\"{hp['learning_rate']:<10.4f} \"\n",
    "              f\"{hp['num_epochs']:<8} \"\n",
    "              f\"{hp['weight_decay']:<10.4f} \"\n",
    "              f\"{hp['dropout_rate']:<10.2f} \"\n",
    "              f\"{result['epoch_train_losses'][-1]:<12.4f} \"\n",
    "              f\"{result['final_accuracy']:<12.2f}%\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# 主程序\n",
    "if __name__ == '__main__':\n",
    "    print(\"开始超参数调优实验...\")\n",
    "    print(\"这将运行多组实验，可能需要较长时间，请耐心等待。\\n\")\n",
    "    \n",
    "    # 运行实验\n",
    "    all_results = run_hyperparameter_experiments(device)\n",
    "    \n",
    "    # 可视化结果\n",
    "    print(\"\\n\\n生成可视化结果...\")\n",
    "    visualize_results(all_results)\n",
    "    \n",
    "    print(\"\\n实验完成！\")\n",
    "    print(\"所有结果已保存在 './hyperparameter_experiments' 目录下\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc0c7f-de11-4344-9108-a6786c37cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
